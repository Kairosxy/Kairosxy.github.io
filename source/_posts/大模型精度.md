# 数值表示基础文档

---

## 一、定点数 与 浮点数（核心区别）

### 1. 什么是定点数（Fixed Point）

**定义**
定点数表示方式的核心特点是：

> **小数点位置是固定的**

计算机内部存的是一个整数，而真实数值由一个**固定的比例关系**解释。

**表示方式（直觉）**

```
真实值 = 存储的整数 × 缩放因子
```

例如规定：

* 缩放因子 = 0.1
* 那么：

  * 存 15 → 表示 1.5
  * 存 1 → 表示 0.1

INT8 / INT16 / INT32 都属于定点数体系。

**本质特点**

* 所有数共用一个尺度
* 精度和范围无法同时兼顾
* 数值分布一旦变化就容易失真

**在神经网络中的问题**

* 神经网络中同一 tensor 内：

  * 可能同时存在非常小的数和非常大的数
* 固定尺度无法适配这种变化

---

### 2. 什么是浮点数（Floating Point）

**定义**
浮点数的核心思想是：

> **允许小数点“浮动”，让数值大小和精细程度分开表示**

**基本结构**

```
[ 符号位 ][ 指数位 ][ 尾数位 ]
```

* 符号位：正 / 负
* 指数位：数量级（决定“有多大”）
* 尾数位：有效数字（决定“有多准”）

**直觉理解**
浮点数本质等价于科学计数法：

```
真实值 ≈ 尾数 × 2^(指数)
```

**优势**

* 能同时表示极小和极大的数
* 非常适合神经网络中的数值波动

---

## 二、浮点数都有哪些（常见与神经网络相关）

从“通用 → 特化 → 低精度”顺序列出。

### 1. FP64（双精度浮点）

* 指数位：多
* 尾数位：多
* 特点：极高精度、极大范围
* 使用场景：科学计算
* 深度学习中：几乎不用

---

### 2. FP32（单精度浮点）

* 指数位：8
* 尾数位：23
* 特点：稳定、通用
* 使用场景：

  * 早期训练
  * 优化器状态
  * 累计计算

---

### 3. TF32（TensorFloat-32）

* NVIDIA 专用内部格式
* 指数位：与 FP32 相同
* 尾数位：显著减少
* 特点：

  * 保留 FP32 的数值范围
  * 用更低精度换取更高吞吐

对用户透明，仅用于 Tensor Core 内部。

---

### 4. FP16（半精度浮点）

* 指数位：5
* 尾数位：10
* 特点：

  * 精度尚可
  * 数值范围较小，容易溢出
* 工程上需要 loss scaling

---

### 5. BF16（Brain Floating Point）

* 指数位：8（与 FP32 相同）
* 尾数位：7
* 特点：

  * 数值范围极稳
  * 精度较粗但可接受
* 目前训练主流格式

---

### 6. FP8（8 位浮点）

FP8 没有唯一格式，常见两种：

**E4M3**

* 指数位：4
* 尾数位：3
* 偏向精度

**E5M2**

* 指数位：5
* 尾数位：2
* 偏向范围

**特点**

* 单独使用几乎不可行
* 必须配合工程级缩放因子
* 用于 GEMM 等高算力算子

---

### 7. FP4（4 位浮点）

* 表示能力极弱
* 强依赖 block scaling
* 主要用于研究和实验

---

### 8. NF4（Normalized Float 4）

* 不是 IEEE 浮点
* 没有指数 / 尾数结构
* 本质是查表编码
* 专为神经网络权重量化设计（QLoRA）

---

### 9. INT8（再次强调）

* 定点数
* 没有指数位
* 依赖缩放因子解释数值
* 常用于推理阶段

---

## 三、浮点数格式是什么（统一视角）

所有标准浮点数都遵循：

```
[ 符号 ][ 指数 ][ 尾数 ]
```

**关键权衡**

* 指数位多 → 数值范围大，不易溢出
* 尾数位多 → 表示精细，误差更小

一句话总结：

> **指数位决定“能不能活”，尾数位决定“活得好不好”。**

---

## 四、指数位的正负是怎么表示的

### 1. 指数位本身没有符号位

这是一个关键点：

> **浮点数中，只有“数值符号位”，没有“指数符号位”。**

指数位始终是一个**非负整数**。

---

### 2. bias（指数偏移量）

为了解释正指数和负指数，引入 bias。

**规则**

```
真实指数 = 存储的指数值 − bias
```

**重要特性**

* bias 不占任何 bit
* bias 写在浮点格式标准中
* 所有硬件自动遵守
* 用户无法修改

**直觉类比**
就像楼层显示：

* 显示 50 被定义为真实 0 层
* 显示值 − 50 = 真实楼层

---

## 五、bias 与 缩放因子（scaling factor）

### 1. bias 是什么

* 属于浮点格式
* 决定指数如何被解释
* 固定、不可变
* 对单个数生效

---

### 2. 缩放因子是什么

**定义**

> **人为引入的整体倍率，用来调整一整个 tensor 的数值范围**

**作用**

* 让数据落入低精度格式的“舒适区”
* 避免溢出
* 提高有效精度利用率

**公式直觉**

```
量化前：x × scale
反量化：结果 ÷ scale
```

---

### 3. 缩放因子是怎么算出来的

工程上采用以下流程：

1. 统计 tensor 中的最大绝对值（amax）
2. 确认目标格式可表示的最大值
3. 计算：

   ```
   scale ≈ 可表示上限 / amax
   ```
4. 所有值统一乘以 scale
5. 转低精度计算
6. 再除以 scale

---

### 4. delayed scaling（延迟缩放）

**问题**

* 单步 amax 可能出现异常峰值
* scale 剧烈变化会破坏精度

**解决方案**

* 维护一个历史窗口（如最近 1024 步）
* 使用窗口内最大 amax
* 提高训练稳定性

---

### 5. bias 与 scaling 的本质区别

| 项目      | bias | scaling   |
| ------- | ---- | --------- |
| 层级      | 浮点格式 | 工程策略      |
| 是否占 bit | 否    | 单独存       |
| 是否可变    | 否    | 是         |
| 作用范围    | 单个数  | 整个 tensor |

一句话区分：

> **bias 是规则，scaling 是手段。**

---

## 结束语

> 定点数靠固定尺度解释世界，浮点数靠指数适配世界；
> 指数的正负由 bias 决定，低精度的生存由 scaling 兜底。


